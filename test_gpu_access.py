#!/usr/bin/env python3
"""Test GPU access with different methods"""

import os
import subprocess

print("üî• Testing GPU Access Methods...")

# Method 1: nvidia-smi
print("\n1. NVIDIA-SMI Test:")
try:
    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free', '--format=csv,noheader'], 
                          capture_output=True, text=True)
    print(f"   ‚úÖ GPU Info: {result.stdout.strip()}")
except Exception as e:
    print(f"   ‚ùå nvidia-smi failed: {e}")

# Method 2: CUDA via ctypes
print("\n2. Direct CUDA Library Test:")
try:
    import ctypes
    # Try different library paths
    cuda_paths = [
        '/usr/lib/x86_64-linux-gnu/libcuda.so.1',
        '/usr/lib/x86_64-linux-gnu/libcuda.so',
        '/usr/local/cuda/lib64/libcuda.so'
    ]
    
    for path in cuda_paths:
        try:
            cuda = ctypes.CDLL(path)
            print(f"   ‚úÖ Loaded CUDA from: {path}")
            
            # Try to get device count
            device_count = ctypes.c_int()
            result = cuda.cuDeviceGetCount(ctypes.byref(device_count))
            if result == 0:
                print(f"   ‚úÖ CUDA devices: {device_count.value}")
            else:
                print(f"   ‚ùå cuDeviceGetCount returned: {result}")
            break
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Failed to load {path}: {e}")
except Exception as e:
    print(f"   ‚ùå CUDA library test failed: {e}")

# Method 3: Check container runtime
print("\n3. Container Runtime Check:")
try:
    # Check if we're in a container
    if os.path.exists('/.dockerenv'):
        print("   üì¶ Running in Docker container")
    elif os.path.exists('/run/.containerenv'):
        print("   üì¶ Running in Podman container")
    
    # Check for NVIDIA runtime
    result = subprocess.run(['nvidia-container-cli', 'info'], capture_output=True, text=True)
    if result.returncode == 0:
        print("   ‚úÖ NVIDIA Container Runtime available")
except:
    pass

# Method 4: Environment diagnostics
print("\n4. Environment Variables:")
cuda_vars = ['CUDA_VISIBLE_DEVICES', 'CUDA_ERROR_LEVEL', 'CUDA_CACHE_PATH', 
             'LD_LIBRARY_PATH', 'NVIDIA_VISIBLE_DEVICES']
for var in cuda_vars:
    val = os.getenv(var)
    if val:
        print(f"   {var}={val}")

# Method 5: Try vLLM
print("\n5. vLLM Test:")
try:
    import vllm
    from vllm import LLM
    print("   ‚úÖ vLLM imported successfully")
    
    # Try to initialize with small model
    try:
        # Use environment variable to skip
        os.environ['VLLM_SKIP_CUDA_DEVICE_COUNT_CHECK'] = '1'
        print("   üîÑ Attempting vLLM initialization...")
        # Don't actually create LLM, just test import
        print("   ‚úÖ vLLM ready (import successful)")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  vLLM init warning: {e}")
except Exception as e:
    print(f"   ‚ùå vLLM not available: {e}")

print("\nüî• GPU Access Test Complete!")